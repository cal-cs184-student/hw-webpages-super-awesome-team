<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: Lionel Verano, Jordan Argel</div>

		<br>

		Link to webpage: <a href="https://cal-cs184-student.github.io/hw-webpages-super-awesome-team/hw3/index.html">cal-cs184-student.github.io/hw-webpages-super-awesome-team/hw3/index.html</a>
		Link to GitHub repository: <a href="https://github.com/cal-cs184-student/hw-webpages-super-awesome-team">Github</a>
		
		<figure style="display: flex; justify-content: center; gap: 20px;">
			<img src="bunny-overview.png" alt="overview" style="width:50%;"/>
			<img src="dragon-overview.png" alt="overview" style="width:50%;"/>
		</figure>

		<h2>Overview</h2>
		This project focuses on fundamental ray-tracing techniques, including Lambertian BRDFs, bounding volume hierarchies (BVH), Monte Carlo estimators for direct and global illumination, and adaptive sampling using confidence intervals.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		Rays were cast from the camera into the scene, treating the camera as a single point and simulating a virtual image plane. For each pixel, multiple samples were taken by generating corresponding rays. The Möller-Trombore algorithm was used to check for intersections between these rays and objects in the scene.

		<h2>Part 2: Bounding Volume Hierarchy (BVH)</h2>
		A BVH was built to efficiently organize scene primitives by splitting them into hierarchical bounding boxes using their midpoints as a heuristic. Ray intersection tests were implemented for both bounding boxes and individual primitives within the BVH structure.

		<h2>Part 3: Direct Illumination</h2>
		A diffuse BSDF was used to model light scattering uniformly across a hemisphere. Zero-bounce illumination (directly from light sources) and one-bounce illumination were implemented using uniform hemisphere sampling and importance sampling for better efficiency in capturing light contributions.

		<h2>Part 4: Global Illumination</h2>
		Recursive bounces were added using indirect illumination to simulate realistic lighting. The implementation allowed for either accumulating multiple bounces (full global illumination) or isolating a specific bounce. Russian Roulette was used to terminate ray bounces, reducing bias while maintaining efficiency probabilistically.

		<h2>Part 5: Adaptive Sampling</h2>
		To improve rendering efficiency, adaptive sampling was applied using a 95% confidence interval. Instead of assigning a fixed number of samples to all pixels, the number of samples was dynamically adjusted based on convergence, allowing faster convergence in low-variance areas while focusing more samples on noisier regions.

		<!-- <h2>(Optional) Part 6: Extra Credit Opportunities</h2>
		No extra credit opportunities were pursued in this project.

		<h2>Additional Notes</h2>
		<ul>
			<li>You can also add code if you'd like as so: <code>code code code</code></li>
			<li>If you'd like to add math equations, 
				<ul>
					<li>You can write inline equations like so: \( a^2 + b^2 = c^2 \)</li>
					<li>You can write display equations like so: \[ a^2 + b^2 = c^2 \]</li>
				</ul>
			</li>
		</ul> -->

		<h2>Part 1: Ray Generation and Primitive Intersection</h2>
		Walk through the ray generation and primitive intersection parts of the rendering pipeline.
		Explain the triangle intersection algorithm you implemented in your own words.
		Show images with normal shading for a few small .dae files.
		To generate rays from the camera, we transform coordinates from image space to camera space and then to world space. The camera is modeled as a pinhole camera, with its position at the origin in camera space and its sensor placed at (0, 0, -1). Given the unrendered corners (x,y) = (0,0) and (1,1), we converted to camera space using the horizontal and vertical field of view parameters (hFov and vFov, which are the view angles along the X and Y axes respectively). After that, we transform the image in camera space and offset our camera sensor by -1 in the Z-coordinate. The diagram summarizes the transformations as shown below:
		\[
		(0,0,-1) \rightarrow (-\tan(hFov/2) \cdot x_{imagespace}, -\tan(hFov/2) \cdot y_{imagespace}, -1)
		\]
		\[
		(1,1,-1) \rightarrow (\tan(hFov/2) \cdot x_{imagespace}, \tan(hFov/2) \cdot y_{imagespace}, -1)
		\]
		<figure>
			<img src="imagetocamera.png" alt="overview" style="width:50%"/>
		</figure>
		Then, we transform the ray’s origin and direction from camera space to world space using the camera-to-world matrix (c2w), allowing us to pass it in with its corresponding origin to the Ray data structure. Lastly, the ray’s valid range is bounded by the near and far clipping planes (nclip and fclip), which are initialized as min_t and max_t, respectively.

		Once rays are generated, the next step is the primitive intersection, where rays are tested for intersections with objects in the scene. For triangle intersections, we used the Möller-Trumbore algorithm. This algorithm computes the intersection of a ray with a triangle by solving for barycentric coordinates (b1, b2) and the ray parameter t. The triangle is defined by its three vertices P0, P1, and P2, and the ray is parameterized as \( o + t \cdot d \), where o is the ray’s origin and d is its direction. The algorithm solves the equation:
		\[
		o + t \cdot d = (1 - b1 - b2)P0 + b1P1 + b2P2
		\]
		If the solution satisfies \( t \in [min_t, max_t] \) and \( b1, b2 \in [0,1] \), the intersection is valid. The surface normal at the intersection point is computed by interpolating the vertex normals using the barycentric coordinates, and the intersection data (including t, the normal, and the material) is stored in the Intersection structure. For sphere intersection, a quadratic equation is solved to find the intersection points. Given a sphere with origin c and radius R, and a ray’s origin o and direction d, the intersection occurs when:
		\[
		(o + t \cdot d - c)^2 - R^2 = 0
		\]
		The smaller valid t within [min_t, max_t] will represent when the ray enters the sphere, and the bigger valid t will represent when the ray exits the sphere. Once we determine its intersection point, we assign its primitives, normal vector from the Möller-Trumbore algorithm parameters, and its BSDF.
		From there, we apply normal shading to a few small .dae files, where the surface normals are directly mapped to RGB colors.

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="CBspheres_lambertian.png" width="400px"/>
				  <figcaption>CBspheres_lambertian.dae</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="CBEmpty.png" width="400px"/>
				  <figcaption>CBEmpty.dae</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="CBbunny.png" width="400px"/>
				  <figcaption>CBbunny.dae</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="CBcoil.png" width="400px"/>
				  <figcaption>CBcoil.dae</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<h2>Part 2: Bounding Volume Hierarchy</h2>
		Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
		Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
		Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.

		The Bounding Volume Hierarchy (BVH) construction algorithm is a critical component of the rendering pipeline, designed to accelerate the ray-primitive intersection test by organizing primitives into a hierarchical tree of bounding boxes. The process begins by considering all primitives within the current bounding box and computing their midpoints. A node is created for a current list of primitives, and if the number of primitives exceeds the max_leaf_size, the bounding box is split along its longest axis, determined by the extent of the bounding box of the primitives’ centroids. The primitives are then partitioned into two groups based on their centroids’ positions relative to the split point. This process is repeated recursively for each group until all nodes become leaf nodes, meaning they contain no more than max_leaf_size primitives. This hierarchical structure allows the renderer to quickly discard rays that do not intersect with a bounding box, significantly reducing the number of intersection tests required.

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="banana.png" width="400px"/>
				  <figcaption>banana.dae</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="dragon.png" width="400px"/>
				  <figcaption>dragon.dae</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="maxplanck.png" width="400px"/>
				  <figcaption>maxplanck.dae</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="beast.png" width="400px"/>
				  <figcaption>beast.dae</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		A comparison of rendering times with and without BVH acceleration highlights the dramatic performance improvement. For example, rendering the banana.dae, dragon.dae, maxplanck.dae, and beast.dae scenes without BVH took me 3567.1536, 3651.2313, 335.6972, and 410.2534 seconds, respectively with my NVIDIA GeForce RTX 3060 Laptop GPU (I went on “AFK” mode). With BVH acceleration, these times dropped to 0.1131, 0.01643, 0.1183, and 0.0825 seconds, respectively. This data demonstrates that BVH acceleration is indispensable for rendering scenes with moderately complex geometries, as it reduces render times by orders of magnitude. The hierarchical structure of the BVH allows the renderer to efficiently cull large portions of the scene that are not intersected by a ray, focusing computational resources only on relevant primitives. This optimization is particularly crucial for real-time rendering and scenes with high geometric complexity.

		<h2>Part 3: Direct Illumination</h2>
		Walk through both implementations of the direct lighting function.
		Show some images rendered with both implementations of the direct lighting function.
		Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
		Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.

		The direct lighting function plays a crucial role in estimating how light interacts with surfaces in a scene. Two approaches were implemented to achieve this: uniform hemisphere sampling and importance sampling of light sources. Both methods rely on Monte Carlo integration to approximate the light contribution at a given point, but they differ in how they sample light directions. The equation of the function is shown below:
		\[
		L_{out}(p, \omega_r) \approx \sum_{j=1}^{N} \frac{L_i(p, \omega_j) \cdot f_r(p, \omega_j \rightarrow \omega_r) \cdot \cos(\theta_j)}{p(\omega_j)}
		\]

		Uniform Hemisphere Sampling generates rays uniformly across a hemisphere centered at the intersection point. For each ray, the algorithm calculates the incoming light \( L_i(p, \omega_j) \) from the sampled direction and evaluates the Bidirectional Reflectance Distribution Function (BRDF) \( f_r(p, \omega_j \rightarrow \omega_r) \) to determine how the light reflects off the surface. The algorithm works as follows:
		<ul>
			<li>A local coordinate system is created at the intersection point, with the surface normal aligned with the Z-axis to allow easier sampling in the hemisphere.</li>
			<li>For each sample, a direction is randomly chosen within the hemisphere using a uniform distribution. The direction is transformed from object space to world space.</li>
			<li>A ray is cast from the intersection point in the sampled direction. The ray’s minimum distance min_t is set to a small epsilon to avoid self-intersection.</li>
			<li>If the ray intersects a light source, the light’s emission is evaluated. The Bidirectional Scattering Distribution Function (BSDF) is used to determine how the light reflects off the surface. The contribution of each sample is weighted by the cosine of the angle between the sampled direction and the surface normal, as well as the probability density function (PDF) of the hemisphere sampling.</li>
			<li>The contribution of all samples is averaged to compute the final radiance.</li>
		</ul>

		While this approach is straightforward, this is an inefficient way of sampling. This is because it generates noisy images, especially in scenes with complex lighting or directional light sources. This is primarily caused by some of the rays that are missing the light sources entirely.

		<figure>
			<img src="1-bouncepath.png" alt="overview" style="width:50%"/>
		</figure>

		In contrast, Importance Sampling focuses on sampling light sources directly, prioritizing those that contribute the most to the scene. Rays are cast from the intersection point toward the light sources, and the algorithm checks for visibility by testing for intersections with other objects. The importance sampling method uses a probability distribution function (PDF) \( p(\omega_j) \) based on the light source’s contribution, ensuring that more significant light sources are sampled more frequently. The algorithm works as follows:
		<ul>
			<li>The algorithm determines the number of samples for each light source in the scene. Only one sample is needed for delta lights (e.g., point lights), while area lights require multiple samples.</li>
			<li>For each sample, the light source is sampled to obtain a direction, distance to the light distToLight, and the probability density function pdf. The direction is transformed from world space to object space.</li>
			<li>A shadow ray is cast from the intersection point toward the light source. The ray’s maximum distance max_t is set to distToLight to avoid overshooting the light. If the ray does not interact with any occluding objects, the light’s contribution is computed.</li>
			<li>The BSDF is evaluated to determine how the light reflects off the surface. The contribution of each sample is weighted by the cosine of the angle between the sampled direction and the surface normal, as well as the PDF of the light sampling.</li>
			<li>The contributions of all samples are averaged to compute the final radiance.</li>
		</ul>

		By increasing the frequency in only those regions that contribute the most to the scene, we can produce cleaner images with less noise and faster convergence, particularly in scenes with area lights or soft shadows.

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="uniform_hemisphere.png" width="400px"/>
				  <figcaption>Uniform Hemisphere Sampling</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="importance_sampling.png" width="400px"/>
				  <figcaption>Light Importance Sampling</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<h2>Comparison of Sampling Methods</h2>
		<p>
			Uniform hemisphere sampling tends to produce noisy images, especially in scenes with complex lighting, because it does not prioritize sampling light sources. 
			In contrast, importance sampling focuses on light sources that contribute the most to the scene, resulting in reduced noise and faster convergence. 
			For example, in the <code>bunny.dae</code>, the uniform hemisphere sampling method produces a grainy background and blurred lighting, 
			while importance sampling yields a crisp, noise-free image with well-defined shadows. 
			In short, importance sampling is better in terms of achieving high-quality renders with fewer samples.
		</p>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="light_1.png" width="400px"/>
				  <figcaption>1 Light Ray</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="light_4.png" width="400px"/>
				  <figcaption>4 Light Rays</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="light_16.png" width="400px"/>
				  <figcaption>16 Light Rays</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="light_64.png" width="400px"/>
				  <figcaption>64 Light Rays</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<h2>Noise Levels in Soft Shadows</h2>
		<p>
			To analyze noise levels, the <code>bunny.dae</code> scene was rendered using importance sampling with 1, 4, 16, and 64 light rays, all with 1 sample per pixel. 
			The results show that as the number of light rays increases, the noise in soft shadows decreases significantly. 
			With 1 light ray, the shadows are scattered and noisy, but with 64 light rays, the shadows become smooth and well-defined. 
			This goes to show the importance of increasing the number of light rays to reduce noise in soft shadows.
		</p>
		<h2>Part 4: Global Illumination</h2>
		<p>Walk through your implementation of the indirect lighting function.</p>
		<p>Global illumination simulates realistic lighting by accounting for both direct and indirect light contributions. While direct lighting computes light arriving directly from light sources, indirect lighting captures light that bounces off surfaces before reaching the camera. This is achieved through recursive ray tracing, where rays are cast from surface intersections to gather light contributions from other surfaces. The indirect lighting function works as follows:</p>
		<ul>
		  <li>At each surface intersection, a new ray is generated with its origin at the intersection point and its direction sampled using the BSDF’s sample_f function. This direction determines where the ray will bounce next.</li>
		  <li>The radiance contribution from the next bounce is computed using the Monte Carlo estimator:
			\[
			L_{out}(p, \omega_r) = \text{OneBounce}(ray_{current}) + \sum_{j=1}^{N} \frac{L_i(p, \omega_j) \cdot f_r(p, \omega_j \rightarrow \omega_r) \cdot \cos(\theta_j)}{p(\omega_j)}
			\]
			where \( L_i(p, \omega_j) \) is the incoming radiance from the next bounce.</li>
		  <li>The recursion continues until the maximum ray depth is reached or the ray does not intersect any surface. At each step, the ray depth is decremented, and the recursion stops when the depth reaches 1.</li>
		  <li>To optimize performance, Russian Roulette is used to probabilistically terminate rays. This prevents infinite recursion while maintaining unbiased results. The termination probability is typically set between [0.30, 0.40], and the radiance contribution is divided by the continuation probability to compensate for the early termination.</li>
		</ul>

		<figure>
			<img src="russian.png">
		</figure>
		
		<p>Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.</p>
		<p>Scenes such as CBspheres_lambertian.dae, blob.dae, wall-e.dae, and bench.dae were rendered with global illumination (direct and indirect lighting) using 1024 samples per pixel. The results demonstrate realistic lighting effects, including color bleeding and soft shadows, which are not achievable with direct illumination alone.</p>
		<figure>
			<img src="cornell2.png">
		</figure>

		<p>Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)</p>
		<p>To compare direct and indirect illumination, the CBspheres_lambertian.dae scene was rendered with each method separately:</p>
		<ul>
		  <li><b>Direct Illumination:</b> Only light arriving directly from light sources is considered. Shadows appear dark and lack detail, as no light bounces are simulated.</li>
		  <li><b>Indirect Illumination:</b> Only light arriving from bounces is considered. The results show color bleeding, where light from the red and blue walls bounces onto the spheres, illuminating their shadows.</li>
		</ul>
		
		<div style="display: flex; flex-direction: column; align-items: center;">
		  <table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
			  <td style="text-align: center;">
				<img src="direct_illumination.png" width="400px"/>
				<figcaption>Only direct illumination</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="indirect_illumination.png" width="400px"/>
				<figcaption>Only indirect illumination</figcaption>
			  </td>
			</tr>
		  </table>
		</div>
		
		<p>For CBbunny.dae, render the mth bounce of light with max_ray_depth set to 0, 1, 2, 3, 4, and 5 (the -m flag), and isAccumBounces=false. Explain in your write-up what you see for the 2nd and 3rd bounce of light, and how it contributes to the quality of the rendered image compared to rasterization. Use 1024 samples per pixel.</p>
		<p>To provide more flexibility in rendering, we also added support for isolating the mth bounce of light instead of accumulating light from all bounces up to the maximum ray depth. This is controlled by the isAccumBounces parameter, which can be set to false to render only the mth bounce. When comparing renders of the second and third bounces, noticeable differences in exposure and lighting quality emerge. The second bounce (max_ray_depth = 2) produces a brighter image compared to the third bounce (max_ray_depth = 3), which appears softer and more diffused. This is because higher bounces simulate light scattering over greater distances, naturally softening shadows and reducing contrast. Ray tracing, with its physically accurate sampling, captures these subtle lighting effects. While rasterization is faster, it sacrifices accuracy in modeling light interactions and surface details. In short, there’s a tradeoff, which highlights the strengths of ray tracing for realistic rendering, even at the cost of increased computational complexity.</p>
		
		<div style="display: flex; flex-direction: column; align-items: center;">
		  <table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
			  <td style="text-align: center;">
				<img src="max_ray_depth_0.png" width="400px"/>
				<figcaption>max_ray_depth = 0</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="max_ray_depth_1.png" width="400px"/>
				<figcaption>max_ray_depth = 1</figcaption>
			  </td>
			</tr>
			<tr>
			  <td style="text-align: center;">
				<img src="max_ray_depth_2.png" width="400px"/>
				<figcaption>max_ray_depth = 2</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="max_ray_depth_3.png" width="400px"/>
				<figcaption>max_ray_depth = 3</figcaption>
			  </td>
			</tr>
			<tr>
			  <td style="text-align: center;">
				<img src="max_ray_depth_4.png" width="400px"/>
				<figcaption>max_ray_depth = 4</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="max_ray_depth_5.png" width="400px"/>
				<figcaption>max_ray_depth = 5</figcaption>
			  </td>
			</tr>
		  </table>
		</div>
		
		<p>Compare rendered views of accumulated and unaccumulated bounces for CBbunny.dae with max_ray_depth set to 0, 1, 2, 3, 4, and 5 (the -m flag). Use 1024 samples per pixel.</p>
		<p>For CBbunny.dae, the m-th bounce of light was rendered with max_ray_depth set to 0, 1, 2, 3, 4, and 5 (isAccumBounces=true, 1024 samples per pixel). Key observations:</p>
		<ul>
		  <li><b>Bounce 0:</b> Only direct light is visible.</li>
		  <li><b>Bounce 1:</b> Light from the first bounce illuminates surfaces directly visible to light sources.</li>
		  <li><b>Bounce 2-5:</b> Higher bounces contribute to softer lighting and color bleeding. As the bounce number increases, we see less noticeable differences.</li>
		</ul>
		
		<div style="display: flex; flex-direction: column; align-items: center;">
		  <table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
			  <td style="text-align: center;">
				<img src="accumulated_max_ray_depth_0.png" width="400px"/>
				<figcaption>max_ray_depth = 0</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="accumulated_max_ray_depth_1.png" width="400px"/>
				<figcaption>max_ray_depth = 1</figcaption>
			  </td>
			</tr>
			<tr>
			  <td style="text-align: center;">
				<img src="accumulated_max_ray_depth_2.png" width="400px"/>
				<figcaption>max_ray_depth = 2</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="accumulated_max_ray_depth_3.png" width="400px"/>
				<figcaption>max_ray_depth = 3</figcaption>
			  </td>
			</tr>
			<tr>
			  <td style="text-align: center;">
				<img src="accumulated_max_ray_depth_4.png" width="400px"/>
				<figcaption>max_ray_depth = 4</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="accumulated_max_ray_depth_5.png" width="400px"/>
				<figcaption>max_ray_depth = 5</figcaption>
			  </td>
			</tr>
		  </table>
		</div>
		
		<p>For CBbunny.dae, output the Russian Roulette rendering with max_ray_depth set to 0, 1, 2, 3, 4, and 100 (the -m flag). Use 1024 samples per pixel.</p>
		<p>We now add Russian Roulette to our implementation for Global Illumination. We can randomly terminate the recursion for ray tracing with a probability of ~0.35 and divide by this probability in the Monte Carlo estimator. By adding Russian Roulette, we can eliminate the sampling bias in our estimator for global illumination. This allows us to render images with a large max ray depth such as 100 a lot faster than before, nearly as quick as rendering images with a max_ray_depth of 4 without it.</p>
		
		<div style="display: flex; flex-direction: column; align-items: center;">
		  <table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
			  <td style="text-align: center;">
				<img src="russian_roulette_max_ray_depth_0.png" width="400px"/>
				<figcaption>max_ray_depth = 0</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="russian_roulette_max_ray_depth_1.png" width="400px"/>
				<figcaption>max_ray_depth = 1</figcaption>
			  </td>
			</tr>
			<tr>
			  <td style="text-align: center;">
				<img src="russian_roulette_max_ray_depth_2.png" width="400px"/>
				<figcaption>max_ray_depth = 2</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="russian_roulette_max_ray_depth_3.png" width="400px"/>
				<figcaption>max_ray_depth = 3</figcaption>
			  </td>
			</tr>
			<tr>
			  <td style="text-align: center;">
				<img src="russian_roulette_max_ray_depth_4.png" width="400px"/>
				<figcaption>max_ray_depth = 4</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="russian_roulette_max_ray_depth_5.png" width="400px"/>
				<figcaption>max_ray_depth = 5</figcaption>
			  </td>
			</tr>
			<tr>
			  <td style="text-align: center;">
				<img src="russian_roulette_max_ray_depth_100.png" width="400px"/>
				<figcaption>max_ray_depth = 100</figcaption>
			  </td>
			</tr>
		  </table>
		</div>
		
		<p>Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.</p>
		<p>With 4 light rays and max_ray_depth = 5, the noise in the image decreases as we increase the sample size. By increasing the samples per pixel, we converge to a better predictor of the actual amount of light being emitted on a particular pixel. Key findings with CBspheres_microfacet_al_ag.dae:</p>
		<ul>
		  <li><b>Low Samples (1-16):</b> The image is noisy due to insufficient sampling.</li>
		  <li><b>High Samples (64-1024):</b> The noise decreases significantly, and the image converges to a more accurate representation of the lighting.</li>
		</ul>
		
		<div style="display: flex; flex-direction: column; align-items: center;">
		  <table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
			  <td style="text-align: center;">
				<img src="1_sample_per_pixel.png" width="400px"/>
				<figcaption>1 sample per pixel</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="2_samples_per_pixel.png" width="400px"/>
				<figcaption>2 samples per pixel</figcaption>
			  </td>
			</tr>
			<tr>
			  <td style="text-align: center;">
				<img src="4_samples_per_pixel.png" width="400px"/>
				<figcaption>4 samples per pixel</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="8_samples_per_pixel.png" width="400px"/>
				<figcaption>8 samples per pixel</figcaption>
			  </td>
			</tr>
			<tr>
			  <td style="text-align: center;">
				<img src="16_samples_per_pixel.png" width="400px"/>
				<figcaption>16 samples per pixel</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="64_samples_per_pixel.png" width="400px"/>
				<figcaption>64 samples per pixel</figcaption>
			  </td>
			</tr>
			<tr>
			  <td style="text-align: center;">
				<img src="1024_samples_per_pixel.png" width="400px"/>
				<figcaption>1024 samples per pixel</figcaption>
			  </td>
			</tr>
		  </table>
		</div>
		
		<p>With that being said, Global illumination enhances realism by simulating both direct and indirect lighting. Recursive ray tracing, combined with Monte Carlo estimation and Russian Roulette, enables efficient computation of light bounces. Higher sample rates and ray depths improve image quality but with diminishing returns. Global illumination has a tradeoff between accuracy and computational cost.</p>
		
		<h2>Part 5: Adaptive Sampling</h2>
		<p>Explain adaptive sampling. Walk through your implementation of the adaptive sampling.</p>
		<p>Adaptive sampling is a technique used in Monte Carlo path tracing to optimize rendering efficiency by dynamically adjusting the number of samples per pixel based on how quickly the pixel’s illuminance converges. Instead of using a fixed, high number of samples for all pixels, adaptive sampling focuses computational resources on regions of the image that require more samples to reduce noise, while terminating sampling early for pixels that have already converged. This approach significantly reduces rendering time without sacrificing image quality. The adaptive sampling algorithm works as follows:</p>
		<ul>
		  <li>For each pixel, the algorithm computes a confidence interval to determine whether the pixel’s illuminance has converged. The confidence interval is given by:
			\[
			I = 1.96 \cdot \frac{\sigma}{\sqrt{n}}
			\]
			where:
			<ul>
			  <li>\(\sigma\) is the standard deviation of the samples,</li>
			  <li>\(n\) is the number of samples taken so far,</li>
			  <li>1.96 is the z-score for a 95% confidence level.</li>
			</ul>
		  </li>
		  <li>The pixel is considered converged if:
			\[
			I \leq \text{maxTolerance} \cdot \mu
			\]
			where \(\mu\) is the mean illuminance of the samples, and maxTolerance is a user-defined threshold (default is 0.05).</li>
		  <li>If the pixel has converged, sampling is terminated early. Otherwise, sampling continues until the maximum number of samples per pixel (ns_aa) is reached.</li>
		  <li>To reduce computational overhead, the convergence check is performed on every samplesPerBatch sample (default is 32), rather than after every sample.</li>
		</ul>
		
		<p>The implementation of adaptive sampling involves the following steps:</p>
		<ul>
		  <li>For each pixel, two variables are maintained:
			<ul>
			  <li>\(s1 = \sum_{k=1}^{n} x_k\), the sum of the sample illuminances,</li>
			  <li>\(s2 = \sum_{k=1}^{n} x_k^2\), the sum of the squares of the sample illuminances.</li>
			</ul>
			These variables allow the mean \(\mu\) and variance \(\sigma^2\) to be computed efficiently.</li>
		  <li>After every samplesPerBatch sample, the confidence interval \(I\) is computed and compared to the convergence condition. If the condition is met, sampling for the pixel is terminated.</li>
		  <li>The actual number of samples used for each pixel is stored in the sampleCountBuffer, which is used to generate the sampling rate image.</li>
		</ul>
		
		<p>Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.</p>
		<p>We will observe in the following images (max_ray_depth = 5, 2048 samples per pixel) that the shadows require more samples, while the flat surfaces need less to converge. This means that non-flat surfaces and shadows exhibit higher sampling rates.</p>
		
		<div style="display: flex; flex-direction: column; align-items: center;">
		  <table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
			  <td style="text-align: center;">
				<img src="adaptive_sampling_render_1.png" width="400px"/>
				<figcaption>Corresponding Render</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="adaptive_sampling_rate_1.png" width="400px"/>
				<figcaption>Sample Rate Visual</figcaption>
			  </td>
			</tr>
			<tr>
			  <td style="text-align: center;">
				<img src="adaptive_sampling_render_2.png" width="400px"/>
				<figcaption>Corresponding Render</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="adaptive_sampling_rate_2.png" width="400px"/>
				<figcaption>Sample Rate Visual</figcaption>
			  </td>
			</tr>
		  </table>
		</div>
		
		<p>By dynamically adjusting the number of samples per pixel, adaptive sampling reduces rendering time without compromising image quality. This makes it an essential technique for efficient and realistic Monte Carlo path tracing.</p>